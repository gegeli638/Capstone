{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "\n",
       "                                            selftext    author  num_comments  \\\n",
       "0  We understand that most people who reply immed...  SQLwitch           175   \n",
       "\n",
       "   is_suicide                                                url  \\\n",
       "0           0  https://www.reddit.com/r/depression/comments/d...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "\n",
       "                                         title_clean author_clean  \\\n",
       "0  broken least understood rule helper may invite...    sql witch   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import model data\n",
    "model_data = pd.read_csv('model_data_.csv', keep_default_na=False)\n",
    "model_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1968 entries, 0 to 1967\n",
      "Data columns (total 10 columns):\n",
      "title             1968 non-null object\n",
      "selftext          1968 non-null object\n",
      "author            1968 non-null object\n",
      "num_comments      1968 non-null int64\n",
      "is_suicide        1968 non-null int64\n",
      "url               1968 non-null object\n",
      "selftext_clean    1968 non-null object\n",
      "title_clean       1968 non-null object\n",
      "author_clean      1968 non-null object\n",
      "megatext_clean    1968 non-null object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 153.8+ KB\n"
     ]
    }
   ],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING X and y\n",
    "X = model_data['megatext_clean'].tolist()\n",
    "y = model_data['is_suicide'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV       \n",
    "#TRAIN-TEST SPLIT\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10666 unique tokens.\n",
      "Shape of data tensor: (1574, 100)\n",
      "Shape of label tensor: (1574,)\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100 #Cuts off reviews after 100 words\n",
    "training_samples = 1500\n",
    "validation_samples = 100\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(Y_train)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 80,801\n",
      "Trainable params: 80,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 4ms/step - loss: 0.6932 - acc: 0.5080 - val_loss: 0.6901 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6785 - acc: 0.6953 - val_loss: 0.6878 - val_acc: 0.5270\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6606 - acc: 0.7993 - val_loss: 0.6833 - val_acc: 0.5676\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6356 - acc: 0.8453 - val_loss: 0.6792 - val_acc: 0.5946\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.6035 - acc: 0.8720 - val_loss: 0.6706 - val_acc: 0.5811\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.5643 - acc: 0.8953 - val_loss: 0.6625 - val_acc: 0.6351\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.5193 - acc: 0.9080 - val_loss: 0.6568 - val_acc: 0.6081\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.4711 - acc: 0.9193 - val_loss: 0.6497 - val_acc: 0.6351\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.4219 - acc: 0.9353 - val_loss: 0.6430 - val_acc: 0.6622\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.3738 - acc: 0.9407 - val_loss: 0.6366 - val_acc: 0.6622\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir = 'glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71187001 -0.34548     0.25773999  1.11580002 -0.45910001 -1.13559997\n",
      " -0.49160001 -0.41088    -0.82639998  0.14788     0.017755    0.4738\n",
      "  0.4341     -0.75437999 -1.1415      0.32315999 -0.10246     0.27882999\n",
      "  0.98781002  1.87709999 -0.85609001 -0.072251    0.79453999  0.32765999\n",
      " -0.29482999 -0.38997999 -0.67232001 -0.18064    -0.57815999 -0.85960001\n",
      "  0.43899    -0.086074   -0.95765001  0.71298999  0.80085999  0.048109\n",
      "  0.09286    -1.01240003  0.13322    -0.25224    -0.26030999 -0.28819001\n",
      " -0.67439002 -1.15820003  0.28542     0.44405001 -0.72180998  0.24398001\n",
      "  1.42970002 -0.2545    ]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix[0]\n",
    "embedding_matrix[1]\n",
    "embedding_matrix.shape\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                160032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 660,065\n",
      "Trainable params: 660,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.7245 - acc: 0.5353 - val_loss: 0.7072 - val_acc: 0.5811\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5753 - acc: 0.6960 - val_loss: 0.7819 - val_acc: 0.5811\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3998 - acc: 0.8400 - val_loss: 0.8763 - val_acc: 0.5405\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2525 - acc: 0.9147 - val_loss: 0.8306 - val_acc: 0.5405\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.1473 - acc: 0.9680 - val_loss: 1.2250 - val_acc: 0.5135\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0992 - acc: 0.9827 - val_loss: 1.3531 - val_acc: 0.4865\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0649 - acc: 0.9900 - val_loss: 1.2516 - val_acc: 0.5135\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0472 - acc: 0.9927 - val_loss: 1.3571 - val_acc: 0.5270\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0354 - acc: 0.9953 - val_loss: 1.4773 - val_acc: 0.5270\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0363 - acc: 0.9953 - val_loss: 1.6424 - val_acc: 0.5270\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step - loss: 1.3012 - acc: 0.5838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.301195502281189, 0.5837563276290894]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "#history = model.fit(x_train, y_train, epochs=4, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 2s 34ms/step - loss: 0.8740 - acc: 0.5287 - val_loss: 0.6826 - val_acc: 0.6486\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 2s 32ms/step - loss: 0.6605 - acc: 0.6073 - val_loss: 0.9538 - val_acc: 0.4324\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.6317 - acc: 0.6667 - val_loss: 0.6430 - val_acc: 0.6486\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.5817 - acc: 0.6933 - val_loss: 0.7963 - val_acc: 0.6757\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.5616 - acc: 0.7220 - val_loss: 0.9486 - val_acc: 0.7432\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 2s 32ms/step - loss: 0.5392 - acc: 0.7540 - val_loss: 1.2966 - val_acc: 0.7162\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.5468 - acc: 0.7673 - val_loss: 0.7025 - val_acc: 0.6486\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.4804 - acc: 0.8060 - val_loss: 0.9925 - val_acc: 0.7027\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.4346 - acc: 0.8240 - val_loss: 1.3114 - val_acc: 0.6892\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 2s 33ms/step - loss: 0.4999 - acc: 0.8293 - val_loss: 0.8795 - val_acc: 0.6351\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size= 32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 0.9676 - acc: 0.6853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9675639271736145, 0.6852791905403137]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

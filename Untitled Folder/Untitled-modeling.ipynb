{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "\n",
       "                                            selftext    author  num_comments  \\\n",
       "0  We understand that most people who reply immed...  SQLwitch           133   \n",
       "\n",
       "   is_suicide                                                url  \\\n",
       "0           0  https://www.reddit.com/r/depression/comments/d...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "\n",
       "                                         title_clean author_clean  \\\n",
       "0  broken least understood rule helper may invite...    sql witch   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import model data\n",
    "model_data = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/data_for_model.csv', keep_default_na=False)\n",
    "model_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING X and y\n",
    "X = model_data['megatext_clean'].tolist()\n",
    "y = model_data['is_suicide'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV       \n",
    "#TRAIN-TEST SPLIT\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10524 unique tokens.\n",
      "Shape of data tensor: (1517, 100)\n",
      "Shape of label tensor: (1517,)\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100 #Cuts off reviews after 100 words\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(Y_train)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 80,801\n",
      "Trainable params: 80,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.6884 - acc: 0.5350 - val_loss: 0.6905 - val_acc: 0.5110\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6577 - acc: 0.7500 - val_loss: 0.6918 - val_acc: 0.5042\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6341 - acc: 0.7400 - val_loss: 0.6949 - val_acc: 0.5072\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6150 - acc: 0.7500 - val_loss: 0.6993 - val_acc: 0.5065\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5978 - acc: 0.7650 - val_loss: 0.6985 - val_acc: 0.5133\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5825 - acc: 0.7800 - val_loss: 0.7022 - val_acc: 0.5156\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5654 - acc: 0.7850 - val_loss: 0.7018 - val_acc: 0.5178\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5490 - acc: 0.7950 - val_loss: 0.7077 - val_acc: 0.5156\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5312 - acc: 0.7950 - val_loss: 0.7045 - val_acc: 0.5178\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5134 - acc: 0.8150 - val_loss: 0.7034 - val_acc: 0.5133\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir = 'glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.11240005e-01  5.87870002e-01  8.98490012e-01 -2.59880006e-01\n",
      "  5.09689987e-01 -5.28739989e-01 -1.13820001e-01  1.83320001e-01\n",
      "  6.99999988e-01  3.06739986e-01  9.56799984e-02 -4.36309987e-04\n",
      " -6.45809993e-02  7.89680004e-01  2.85919994e-01 -3.92529994e-01\n",
      "  4.17169988e-01 -4.59399998e-01 -3.52809995e-01 -4.24270004e-01\n",
      "  4.93409991e-01 -3.44240010e-01  3.26229990e-01 -2.72870004e-01\n",
      " -1.28390002e+00  3.13259989e-01  1.27579999e+00  5.84449992e-02\n",
      "  1.35409996e-01 -3.33370000e-01 -1.69129997e-01  3.35090011e-01\n",
      "  8.24100003e-02  3.52510005e-01 -5.08639991e-01  2.15079993e-01\n",
      " -5.62960029e-01  1.02839994e+00 -1.12070002e-01 -7.91029990e-01\n",
      " -2.78470010e-01  8.27870011e-01 -1.44079998e-01 -6.15670025e-01\n",
      "  7.01359987e-01 -7.62320012e-02 -3.74300003e-01  5.90250015e-01\n",
      " -3.37879993e-02  6.65279984e-01 -1.91499996e+00  2.06359997e-01\n",
      "  3.43040004e-02 -5.57929985e-02 -4.93160009e-01 -1.33959994e-01\n",
      "  4.52609986e-01  3.70240003e-01 -7.00030010e-03 -3.26289982e-02\n",
      " -7.27760017e-01 -6.90519989e-01 -8.03219974e-01 -2.42660001e-01\n",
      "  1.99729994e-01  1.76809996e-01  7.59739995e-01  1.93629995e-01\n",
      " -7.67939985e-01  4.75360006e-01 -3.80180001e-01  7.34420002e-01\n",
      " -1.09449995e+00 -1.21109998e+00 -7.38869980e-02  3.21790010e-01\n",
      "  1.56639993e-01  3.39839995e-01 -1.12399995e+00  1.31799996e-01\n",
      " -4.75129992e-01 -7.80939996e-01 -1.77579999e-01 -6.03600025e-01\n",
      "  3.86339992e-01 -6.98620006e-02 -5.56529999e-01 -5.03100008e-02\n",
      "  1.30179999e-02  4.41300005e-01  1.34480000e-01 -1.83840003e-02\n",
      " -3.42079997e-01  5.22539973e-01 -3.14559996e-01 -4.55110013e-01\n",
      " -8.33680034e-02 -4.93539989e-01 -1.28780007e-01 -4.57639992e-01]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix[0]\n",
    "embedding_matrix[1]\n",
    "embedding_matrix.shape\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.0274 - acc: 0.6050 - val_loss: 0.8409 - val_acc: 0.5140\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.4452 - acc: 0.7750 - val_loss: 0.8014 - val_acc: 0.5634\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2365 - acc: 0.9200 - val_loss: 0.7831 - val_acc: 0.5619\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1313 - acc: 0.9800 - val_loss: 0.7706 - val_acc: 0.5923\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0677 - acc: 1.0000 - val_loss: 1.1281 - val_acc: 0.5194\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0452 - acc: 1.0000 - val_loss: 1.1921 - val_acc: 0.5232\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.9321 - val_acc: 0.5642\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.8881 - val_acc: 0.5892\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 1.1190 - val_acc: 0.5528\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 1.0972 - val_acc: 0.5634\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 1.1386 - acc: 0.5526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1386477947235107, 0.5526315569877625]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.7052 - acc: 0.4750 - val_loss: 0.7058 - val_acc: 0.5103\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.5586 - acc: 0.7300 - val_loss: 0.6927 - val_acc: 0.5216\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3690 - acc: 0.9950 - val_loss: 0.6980 - val_acc: 0.5361\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.2436 - acc: 1.0000 - val_loss: 0.7019 - val_acc: 0.5399\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1748 - acc: 1.0000 - val_loss: 0.7099 - val_acc: 0.5361\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1300 - acc: 1.0000 - val_loss: 0.7238 - val_acc: 0.5330\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1019 - acc: 1.0000 - val_loss: 0.7491 - val_acc: 0.5194\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0839 - acc: 1.0000 - val_loss: 0.7501 - val_acc: 0.5300\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0766 - acc: 1.0000 - val_loss: 0.7513 - val_acc: 0.5232\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0722 - acc: 1.0000 - val_loss: 0.7515 - val_acc: 0.5262\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 0.6922 - acc: 0.4938 - val_loss: 0.6845 - val_acc: 0.6250\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6689 - acc: 0.8188 - val_loss: 0.6663 - val_acc: 0.6000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6343 - acc: 0.7250 - val_loss: 0.6343 - val_acc: 0.6750\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5653 - acc: 0.8500 - val_loss: 0.5779 - val_acc: 0.6500\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5001 - acc: 0.7750 - val_loss: 0.5535 - val_acc: 0.7500\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4243 - acc: 0.8813 - val_loss: 0.5509 - val_acc: 0.7500\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3726 - acc: 0.8813 - val_loss: 0.5281 - val_acc: 0.7250\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3157 - acc: 0.9500 - val_loss: 0.5210 - val_acc: 0.7250\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2671 - acc: 0.9625 - val_loss: 0.5262 - val_acc: 0.7250\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2355 - acc: 0.9812 - val_loss: 0.5268 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

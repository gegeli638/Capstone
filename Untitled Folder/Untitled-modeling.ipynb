{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "\n",
       "                                            selftext    author  num_comments  \\\n",
       "0  We understand that most people who reply immed...  SQLwitch           133   \n",
       "\n",
       "   is_suicide                                                url  \\\n",
       "0           0  https://www.reddit.com/r/depression/comments/d...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "\n",
       "                                         title_clean author_clean  \\\n",
       "0  broken least understood rule helper may invite...    sql witch   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import model data\n",
    "model_data = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/data_for_model.csv', keep_default_na=False)\n",
    "model_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING X and y\n",
    "X = model_data['megatext_clean'].tolist()\n",
    "y = model_data['is_suicide'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV       \n",
    "#TRAIN-TEST SPLIT\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10524 unique tokens.\n",
      "Shape of data tensor: (1517, 100)\n",
      "Shape of label tensor: (1517,)\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100 #Cuts off reviews after 100 words\n",
    "training_samples = 1500\n",
    "validation_samples = 50\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(Y_train)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir = 'glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71187001 -0.34548     0.25773999  1.11580002 -0.45910001 -1.13559997\n",
      " -0.49160001 -0.41088    -0.82639998  0.14788     0.017755    0.4738\n",
      "  0.4341     -0.75437999 -1.1415      0.32315999 -0.10246     0.27882999\n",
      "  0.98781002  1.87709999 -0.85609001 -0.072251    0.79453999  0.32765999\n",
      " -0.29482999 -0.38997999 -0.67232001 -0.18064    -0.57815999 -0.85960001\n",
      "  0.43899    -0.086074   -0.95765001  0.71298999  0.80085999  0.048109\n",
      "  0.09286    -1.01240003  0.13322    -0.25224    -0.26030999 -0.28819001\n",
      " -0.67439002 -1.15820003  0.28542     0.44405001 -0.72180998  0.24398001\n",
      "  1.42970002 -0.2545    ]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix[0]\n",
    "embedding_matrix[1]\n",
    "embedding_matrix.shape\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 660,065\n",
      "Trainable params: 660,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "#model.add(LSTM(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.6882 - acc: 0.5687 - val_loss: 0.6165 - val_acc: 0.7647\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5670 - acc: 0.7100 - val_loss: 0.7082 - val_acc: 0.5882\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3790 - acc: 0.8660 - val_loss: 0.5829 - val_acc: 0.6471\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2455 - acc: 0.9233 - val_loss: 0.6997 - val_acc: 0.7059\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.1356 - acc: 0.9713 - val_loss: 0.8628 - val_acc: 0.5882\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 10ms/step - loss: 0.0858 - acc: 0.9807 - val_loss: 0.8378 - val_acc: 0.6471\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0588 - acc: 0.9913 - val_loss: 0.9177 - val_acc: 0.6471\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0374 - acc: 0.9960 - val_loss: 1.4265 - val_acc: 0.5294\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.0252 - acc: 0.9967 - val_loss: 1.1582 - val_acc: 0.6471\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0232 - acc: 0.9980 - val_loss: 1.2698 - val_acc: 0.6471\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.4226 - acc: 0.5632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.422588586807251, 0.5631579160690308]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                10624     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 510,657\n",
      "Trainable params: 510,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 1s 39ms/step - loss: 0.6797 - acc: 0.5650 - val_loss: 0.6610 - val_acc: 0.5867\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.6190 - acc: 0.7183 - val_loss: 0.6289 - val_acc: 0.6600\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.4936 - acc: 0.8025 - val_loss: 0.6613 - val_acc: 0.6400\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 1s 36ms/step - loss: 0.3223 - acc: 0.8925 - val_loss: 0.6658 - val_acc: 0.6700\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.2082 - acc: 0.9425 - val_loss: 0.7077 - val_acc: 0.6433\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.1495 - acc: 0.9633 - val_loss: 0.7482 - val_acc: 0.6733\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.0995 - acc: 0.9775 - val_loss: 0.8096 - val_acc: 0.6500\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.0593 - acc: 0.9917 - val_loss: 0.9474 - val_acc: 0.6700\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 1s 37ms/step - loss: 0.0546 - acc: 0.9867 - val_loss: 0.9041 - val_acc: 0.6633\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 1s 29ms/step - loss: 0.0359 - acc: 0.9908 - val_loss: 1.0328 - val_acc: 0.6567\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "#model.add(Embedding(10000,32))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 7.4804 - acc: 0.4820 - val_loss: 2.6296 - val_acc: 0.5882\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.6514 - acc: 0.4833 - val_loss: 0.8389 - val_acc: 0.4118\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9863 - acc: 0.4840 - val_loss: 0.8504 - val_acc: 0.3529\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8899 - acc: 0.5053 - val_loss: 0.8322 - val_acc: 0.4118\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8646 - acc: 0.5080 - val_loss: 0.8341 - val_acc: 0.3529\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8577 - acc: 0.5113 - val_loss: 0.8317 - val_acc: 0.3529\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8526 - acc: 0.5080 - val_loss: 0.8292 - val_acc: 0.3529\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8481 - acc: 0.5093 - val_loss: 0.8261 - val_acc: 0.4118\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8440 - acc: 0.5160 - val_loss: 0.8228 - val_acc: 0.4118\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8338 - acc: 0.5160 - val_loss: 0.8294 - val_acc: 0.4118\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size= 1517, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 8ms/step - loss: 0.7867 - acc: 0.4921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7867146730422974, 0.49210527539253235]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-bert\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
      "Requirement already satisfied: numpy in d:\\programming\\9.3\\lib\\site-packages (from keras-bert) (1.16.4)\n",
      "Requirement already satisfied: Keras>=2.4.3 in d:\\programming\\9.3\\lib\\site-packages (from keras-bert) (2.4.3)\n",
      "Collecting keras-transformer>=0.38.0 (from keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
      "Requirement already satisfied: scipy>=0.14 in d:\\programming\\9.3\\lib\\site-packages (from Keras>=2.4.3->keras-bert) (1.2.1)\n",
      "Requirement already satisfied: h5py in d:\\programming\\9.3\\lib\\site-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in d:\\programming\\9.3\\lib\\site-packages (from Keras>=2.4.3->keras-bert) (5.1.1)\n",
      "Collecting keras-pos-embd>=0.11.0 (from keras-transformer>=0.38.0->keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
      "Collecting keras-multi-head>=0.27.0 (from keras-transformer>=0.38.0->keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
      "Collecting keras-layer-normalization>=0.14.0 (from keras-transformer>=0.38.0->keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
      "Collecting keras-position-wise-feed-forward>=0.6.0 (from keras-transformer>=0.38.0->keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
      "Collecting keras-embed-sim>=0.8.0 (from keras-transformer>=0.38.0->keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
      "Requirement already satisfied: six in d:\\programming\\9.3\\lib\\site-packages (from h5py->Keras>=2.4.3->keras-bert) (1.12.0)\n",
      "Collecting keras-self-attention==0.46.0 (from keras-multi-head>=0.27.0->keras-transformer>=0.38.0->keras-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
      "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
      "  Building wheel for keras-bert (setup.py): started\n",
      "  Building wheel for keras-bert (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\66\\f0\\b1\\748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
      "  Building wheel for keras-transformer (setup.py): started\n",
      "  Building wheel for keras-transformer (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\e5\\fb\\3a\\37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
      "  Building wheel for keras-pos-embd (setup.py): started\n",
      "  Building wheel for keras-pos-embd (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\5b\\a1\\a0\\ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
      "  Building wheel for keras-multi-head (setup.py): started\n",
      "  Building wheel for keras-multi-head (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\b5\\b4\\49\\0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
      "  Building wheel for keras-layer-normalization (setup.py): started\n",
      "  Building wheel for keras-layer-normalization (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\54\\80\\22\\a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py): started\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\39\\e2\\e2\\3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
      "  Building wheel for keras-embed-sim (setup.py): started\n",
      "  Building wheel for keras-embed-sim (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\49\\45\\8b\\c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
      "  Building wheel for keras-self-attention (setup.py): started\n",
      "  Building wheel for keras-self-attention (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gegeli\\AppData\\Local\\pip\\Cache\\wheels\\d2\\2e\\80\\fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
      "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
      "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
      "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

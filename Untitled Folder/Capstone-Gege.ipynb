{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone - \n",
    "# Detecting Suicidal Trend Posts with Deep Learning(LSTM) Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Data Source and Data Cleaning\n",
    "\n",
    "The original data is from two subreddits on Reddit, \n",
    "-\tr/depression: because nobody should be alone in a dark place (https://www.reddit.com/r/depression/)  \n",
    "-\tr/SuicideWatch: Peer support for anyone struggling with suicidal thoughts. (https://www.reddit.com/r/SuicideWatch/). \n",
    "\n",
    "The original code(https://github.com/hesamuel/goodbye_world/blob/master/code/01_Data_Collection.ipynb ) use Redditâ€™s API to collect data. \n",
    "\n",
    "Set a new column named __'is_suicide'__ manually as labled.  \n",
    "- 'is_suicide' = 0: all the posts from depression subreddits. No suicidal trend\n",
    "- 'is_suicide' = 1: all the posts from SuicideWatch subreddits. Have a suicidal trend\n",
    "\n",
    "In order to make sure we have the same original dataset for further analytics and comparation, I download the csv file directly from the original code and read it from my GitHub.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>hidden</th>\n",
       "      <th>pwls</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>downs</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>name</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>category</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>score</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>edited</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>gildings</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>is_self</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>created</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>wls</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>domain</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>likes</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>view_count</th>\n",
       "      <th>archived</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>pinned</th>\n",
       "      <th>over_18</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>awarders</th>\n",
       "      <th>media_only</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>locked</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>visited</th>\n",
       "      <th>removed_by</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>id</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>author</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>permalink</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>is_suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>depression</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>t2_1t70</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>r/depression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_doqwow</td>\n",
       "      <td>False</td>\n",
       "      <td>dark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>public</td>\n",
       "      <td>1818</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1818</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.572390e+09</td>\n",
       "      <td>text</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>self.depression</td>\n",
       "      <td>False</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>confidence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moderator</td>\n",
       "      <td>t5_2qqqf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>doqwow</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133</td>\n",
       "      <td>True</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/depression/comments/doqwow/our_mostbroken_a...</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>True</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>611580</td>\n",
       "      <td>1.572361e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   approved_at_utc   subreddit  \\\n",
       "0              NaN  depression   \n",
       "\n",
       "                                            selftext author_fullname  saved  \\\n",
       "0  We understand that most people who reply immed...         t2_1t70  False   \n",
       "\n",
       "   mod_reason_title  gilded  clicked  \\\n",
       "0               NaN       0    False   \n",
       "\n",
       "                                               title link_flair_richtext  \\\n",
       "0  Our most-broken and least-understood rules is ...                  []   \n",
       "\n",
       "  subreddit_name_prefixed  hidden  pwls  link_flair_css_class  downs  \\\n",
       "0            r/depression   False   0.0                   NaN      0   \n",
       "\n",
       "   hide_score       name  quarantine link_flair_text_color  \\\n",
       "0       False  t3_doqwow       False                  dark   \n",
       "\n",
       "   author_flair_background_color subreddit_type   ups  total_awards_received  \\\n",
       "0                            NaN         public  1818                      0   \n",
       "\n",
       "  media_embed  author_flair_template_id  is_original_content user_reports  \\\n",
       "0          {}                       NaN                False           []   \n",
       "\n",
       "   secure_media  is_reddit_media_domain  is_meta  category secure_media_embed  \\\n",
       "0           NaN                   False    False       NaN                 {}   \n",
       "\n",
       "   link_flair_text  can_mod_post  score  approved_by author_premium  \\\n",
       "0              NaN         False   1818          NaN           True   \n",
       "\n",
       "   thumbnail edited  author_flair_css_class author_flair_richtext gildings  \\\n",
       "0        NaN  False                     NaN                    []       {}   \n",
       "\n",
       "   content_categories  is_self  mod_note       created link_flair_type  wls  \\\n",
       "0                 NaN     True       NaN  1.572390e+09            text  0.0   \n",
       "\n",
       "   removed_by_category  banned_by author_flair_type           domain  \\\n",
       "0                  NaN        NaN              text  self.depression   \n",
       "\n",
       "   allow_live_comments                                      selftext_html  \\\n",
       "0                False  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...   \n",
       "\n",
       "   likes suggested_sort  banned_at_utc  view_count  archived  no_follow  \\\n",
       "0    NaN     confidence            NaN         NaN     False      False   \n",
       "\n",
       "   is_crosspostable  pinned  over_18 all_awardings awarders  media_only  \\\n",
       "0             False   False    False            []       []       False   \n",
       "\n",
       "   can_gild  spoiler  locked  author_flair_text  visited  removed_by  \\\n",
       "0     False    False   False                NaN    False         NaN   \n",
       "\n",
       "   num_reports distinguished subreddit_id  mod_reason_by  removal_reason  \\\n",
       "0          NaN     moderator     t5_2qqqf            NaN             NaN   \n",
       "\n",
       "   link_flair_background_color      id  is_robot_indexable  report_reasons  \\\n",
       "0                          NaN  doqwow                True             NaN   \n",
       "\n",
       "     author  discussion_type  num_comments  send_replies whitelist_status  \\\n",
       "0  SQLwitch              NaN           133          True           no_ads   \n",
       "\n",
       "   contest_mode mod_reports author_patreon_flair author_flair_text_color  \\\n",
       "0         False          []                False                     NaN   \n",
       "\n",
       "                                           permalink parent_whitelist_status  \\\n",
       "0  /r/depression/comments/doqwow/our_mostbroken_a...                  no_ads   \n",
       "\n",
       "   stickied                                                url  \\\n",
       "0      True  https://www.reddit.com/r/depression/comments/d...   \n",
       "\n",
       "   subreddit_subscribers   created_utc  num_crossposts  media  is_video  \\\n",
       "0                 611580  1.572361e+09               0    NaN     False   \n",
       "\n",
       "   is_suicide  \n",
       "0           0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing depression CSV files\n",
    "depression = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/depression.csv')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "depression.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>hidden</th>\n",
       "      <th>pwls</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>downs</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>name</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>category</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>score</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>edited</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>gildings</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>is_self</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>created</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>wls</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>domain</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>likes</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>view_count</th>\n",
       "      <th>archived</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>pinned</th>\n",
       "      <th>over_18</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>awarders</th>\n",
       "      <th>media_only</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>locked</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>visited</th>\n",
       "      <th>removed_by</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>id</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>author</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>permalink</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>is_suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>SuicideWatch</td>\n",
       "      <td>We've been seeing a worrying increase in pro-s...</td>\n",
       "      <td>t2_1t70</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>New wiki on how to avoid accidentally encourag...</td>\n",
       "      <td>[]</td>\n",
       "      <td>r/SuicideWatch</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_cz6nfd</td>\n",
       "      <td>False</td>\n",
       "      <td>dark</td>\n",
       "      <td>NaN</td>\n",
       "      <td>public</td>\n",
       "      <td>1728</td>\n",
       "      <td>2</td>\n",
       "      <td>{}</td>\n",
       "      <td>2a7b5518-8e45-11e5-a506-0ed10b342609</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1567537996.0</td>\n",
       "      <td>modmsg</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'gid_1': 1, 'gid_2': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.567555e+09</td>\n",
       "      <td>text</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>self.SuicideWatch</td>\n",
       "      <td>True</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'count': 1, 'is_enabled': True, 'subreddit_i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moderator</td>\n",
       "      <td>t5_2qpzs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cz6nfd</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260</td>\n",
       "      <td>True</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>dark</td>\n",
       "      <td>/r/SuicideWatch/comments/cz6nfd/new_wiki_on_ho...</td>\n",
       "      <td>no_ads</td>\n",
       "      <td>True</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>188579</td>\n",
       "      <td>1.567526e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   approved_at_utc     subreddit  \\\n",
       "0              NaN  SuicideWatch   \n",
       "\n",
       "                                            selftext author_fullname  saved  \\\n",
       "0  We've been seeing a worrying increase in pro-s...         t2_1t70  False   \n",
       "\n",
       "   mod_reason_title  gilded  clicked  \\\n",
       "0               NaN       1    False   \n",
       "\n",
       "                                               title link_flair_richtext  \\\n",
       "0  New wiki on how to avoid accidentally encourag...                  []   \n",
       "\n",
       "  subreddit_name_prefixed  hidden  pwls  link_flair_css_class  downs  \\\n",
       "0          r/SuicideWatch   False     0                   NaN      0   \n",
       "\n",
       "   hide_score       name  quarantine link_flair_text_color  \\\n",
       "0       False  t3_cz6nfd       False                  dark   \n",
       "\n",
       "   author_flair_background_color subreddit_type   ups  total_awards_received  \\\n",
       "0                            NaN         public  1728                      2   \n",
       "\n",
       "  media_embed              author_flair_template_id  is_original_content  \\\n",
       "0          {}  2a7b5518-8e45-11e5-a506-0ed10b342609                False   \n",
       "\n",
       "  user_reports  secure_media  is_reddit_media_domain  is_meta  category  \\\n",
       "0           []           NaN                   False    False       NaN   \n",
       "\n",
       "  secure_media_embed  link_flair_text  can_mod_post  score  approved_by  \\\n",
       "0                 {}              NaN         False   1728          NaN   \n",
       "\n",
       "  author_premium  thumbnail        edited author_flair_css_class  \\\n",
       "0           True        NaN  1567537996.0                 modmsg   \n",
       "\n",
       "  author_flair_richtext                  gildings  content_categories  \\\n",
       "0                    []  {'gid_1': 1, 'gid_2': 1}                 NaN   \n",
       "\n",
       "   is_self  mod_note       created link_flair_type  wls  removed_by_category  \\\n",
       "0     True       NaN  1.567555e+09            text    0                  NaN   \n",
       "\n",
       "   banned_by author_flair_type             domain  allow_live_comments  \\\n",
       "0        NaN              text  self.SuicideWatch                 True   \n",
       "\n",
       "                                       selftext_html  likes  suggested_sort  \\\n",
       "0  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...    NaN             NaN   \n",
       "\n",
       "   banned_at_utc  view_count  archived  no_follow  is_crosspostable  pinned  \\\n",
       "0            NaN         NaN      True      False             False   False   \n",
       "\n",
       "   over_18                                      all_awardings awarders  \\\n",
       "0    False  [{'count': 1, 'is_enabled': True, 'subreddit_i...       []   \n",
       "\n",
       "   media_only  can_gild  spoiler  locked  author_flair_text  visited  \\\n",
       "0       False     False    False   False                NaN    False   \n",
       "\n",
       "   removed_by  num_reports distinguished subreddit_id  mod_reason_by  \\\n",
       "0         NaN          NaN     moderator     t5_2qpzs            NaN   \n",
       "\n",
       "   removal_reason  link_flair_background_color      id  is_robot_indexable  \\\n",
       "0             NaN                          NaN  cz6nfd                True   \n",
       "\n",
       "   report_reasons    author  discussion_type  num_comments  send_replies  \\\n",
       "0             NaN  SQLwitch              NaN           260          True   \n",
       "\n",
       "  whitelist_status  contest_mode mod_reports author_patreon_flair  \\\n",
       "0           no_ads         False          []                False   \n",
       "\n",
       "  author_flair_text_color                                          permalink  \\\n",
       "0                    dark  /r/SuicideWatch/comments/cz6nfd/new_wiki_on_ho...   \n",
       "\n",
       "  parent_whitelist_status  stickied  \\\n",
       "0                  no_ads      True   \n",
       "\n",
       "                                                 url  subreddit_subscribers  \\\n",
       "0  https://www.reddit.com/r/SuicideWatch/comments...                 188579   \n",
       "\n",
       "    created_utc  num_crossposts  media  is_video author_cakeday  is_suicide  \n",
       "0  1.567526e+09               0    NaN     False            NaN           1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing suicide_watch CSV files\n",
    "suicide_watch = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/suicide_watch.csv')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "suicide_watch.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all this columns in depression dataset, we only need: \n",
    "- \"title\": the title of the post\n",
    "- \"selftext\": the content of the post\n",
    "- \"author\": the author of the post\n",
    "- \"num_comments\": how many comments each post has\n",
    "- \"is_suicide\": whether the post have a suicidal trend (0 is from depression, 1 is from suicidewatch)\n",
    "- \"url\": the original url of the post\n",
    "\n",
    "Select these column then combine depression and SuicideWatch dataset together. Now we get the combined data. In order to make sure we have the same original dataset for further analytics and comparation, I download the csv file directly from the original code and read it from my GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regular Check-In Post</td>\n",
       "      <td>Welcome to /r/depression's check-in post - a p...</td>\n",
       "      <td>circinia</td>\n",
       "      <td>1644</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I hate it so much when you try and express you...</td>\n",
       "      <td>I've been feeling really depressed and lonely ...</td>\n",
       "      <td>TheNewKiller69</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "1                              Regular Check-In Post   \n",
       "2  I hate it so much when you try and express you...   \n",
       "\n",
       "                                            selftext          author  \\\n",
       "0  We understand that most people who reply immed...        SQLwitch   \n",
       "1  Welcome to /r/depression's check-in post - a p...        circinia   \n",
       "2  I've been feeling really depressed and lonely ...  TheNewKiller69   \n",
       "\n",
       "   num_comments  is_suicide                                                url  \n",
       "0           133           0  https://www.reddit.com/r/depression/comments/d...  \n",
       "1          1644           0  https://www.reddit.com/r/depression/comments/e...  \n",
       "2             8           0  https://www.reddit.com/r/depression/comments/f...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing combined_data CSV files\n",
    "combined_data = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/combined_data.csv')\n",
    "combined_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897 entries, 0 to 1896\n",
      "Data columns (total 6 columns):\n",
      "title           1897 non-null object\n",
      "selftext        1832 non-null object\n",
      "author          1897 non-null object\n",
      "num_comments    1897 non-null int64\n",
      "is_suicide      1897 non-null int64\n",
      "url             1897 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 89.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#show the combined_data info and check the null value\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the __Data Cleaning__ part. First of all, check the null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Can someone recommend good qualities in therap...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eito_8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>Nobody gives a fuck until you die, and even th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lil_peemis</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>I have two brothers who have killed themselves...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ArsenalOwl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>I want to die I want to die I want to die</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alynde</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>I am so sorry, but it has gotten worse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SmushyKidK</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title selftext      author  \\\n",
       "184  Can someone recommend good qualities in therap...      NaN      eito_8   \n",
       "921  Nobody gives a fuck until you die, and even th...      NaN  lil_peemis   \n",
       "923  I have two brothers who have killed themselves...      NaN  ArsenalOwl   \n",
       "925          I want to die I want to die I want to die      NaN      alynde   \n",
       "934             I am so sorry, but it has gotten worse      NaN  SmushyKidK   \n",
       "\n",
       "     num_comments  is_suicide  \\\n",
       "184             2           0   \n",
       "921             3           1   \n",
       "923             1           1   \n",
       "925             4           1   \n",
       "934             4           1   \n",
       "\n",
       "                                                   url  \n",
       "184  https://www.reddit.com/r/depression/comments/f...  \n",
       "921  https://www.reddit.com/r/SuicideWatch/comments...  \n",
       "923  https://www.reddit.com/r/SuicideWatch/comments...  \n",
       "925  https://www.reddit.com/r/SuicideWatch/comments...  \n",
       "934  https://www.reddit.com/r/SuicideWatch/comments...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are only null values in selftext column\n",
    "#checking out the rows with missing selftext data\n",
    "combined_data[combined_data[\"selftext\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do further analytics, fill the null value as 'emptypost'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Can someone recommend good qualities in therap...</td>\n",
       "      <td>emptypost</td>\n",
       "      <td>eito_8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>Nobody gives a fuck until you die, and even th...</td>\n",
       "      <td>emptypost</td>\n",
       "      <td>lil_peemis</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>I have two brothers who have killed themselves...</td>\n",
       "      <td>emptypost</td>\n",
       "      <td>ArsenalOwl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>I want to die I want to die I want to die</td>\n",
       "      <td>emptypost</td>\n",
       "      <td>alynde</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>I am so sorry, but it has gotten worse</td>\n",
       "      <td>emptypost</td>\n",
       "      <td>SmushyKidK</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   selftext      author  \\\n",
       "184  Can someone recommend good qualities in therap...  emptypost      eito_8   \n",
       "921  Nobody gives a fuck until you die, and even th...  emptypost  lil_peemis   \n",
       "923  I have two brothers who have killed themselves...  emptypost  ArsenalOwl   \n",
       "925          I want to die I want to die I want to die  emptypost      alynde   \n",
       "934             I am so sorry, but it has gotten worse  emptypost  SmushyKidK   \n",
       "\n",
       "     num_comments  is_suicide  \\\n",
       "184             2           0   \n",
       "921             3           1   \n",
       "923             1           1   \n",
       "925             4           1   \n",
       "934             4           1   \n",
       "\n",
       "                                                   url  \n",
       "184  https://www.reddit.com/r/depression/comments/f...  \n",
       "921  https://www.reddit.com/r/SuicideWatch/comments...  \n",
       "923  https://www.reddit.com/r/SuicideWatch/comments...  \n",
       "925  https://www.reddit.com/r/SuicideWatch/comments...  \n",
       "934  https://www.reddit.com/r/SuicideWatch/comments...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filling NaN\n",
    "combined_data[\"selftext\"].fillna(\"emptypost\",inplace=True)\n",
    "#check\n",
    "combined_data[combined_data[\"selftext\"].isin([\"emptypost\"])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Pre-processing\n",
    "Using NLP methods to deal with the text data before modeling part.\n",
    "\n",
    "Building a processing function named 'processing_text'. That function can help \n",
    "- change the text to lowercase\n",
    "- remove punctuations\n",
    "- reduce related words down to a common base word. \n",
    "\n",
    "With this functions, we can create a seperate column for our clean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cleaning selftext and title\n",
    "def processing_text(series_to_process):\n",
    "    new_list = []\n",
    "    tokenizer = RegexpTokenizer(r'(\\w+)')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(series_to_process)):\n",
    "        #TOKENISED ITEM(LONG STRING) IN A LIST\n",
    "        dirty_string = (series_to_process)[i].lower()\n",
    "        words_only = tokenizer.tokenize(dirty_string) #WORDS_ONLY IS A LIST THAT DOESN'T HAVE PUNCTUATION\n",
    "        #LEMMATISE THE ITEMS IN WORDS_ONLY\n",
    "        words_only_lem = [lemmatizer.lemmatize(i) for i in words_only]\n",
    "        #REMOVING STOP WORDS FROM THE LEMMATIZED LIST\n",
    "        words_without_stop = [i for i in words_only_lem if i not in stopwords.words(\"english\")]\n",
    "        #RETURN SEPERATED WORDS INTO LONG STRING\n",
    "        long_string_clean = \" \".join(word for word in words_without_stop)\n",
    "        new_list.append(long_string_clean)\n",
    "    return new_list\n",
    "\n",
    "#for cleaning author\n",
    "def processing_author_names(series_to_process):\n",
    "    author_split = []\n",
    "    for i in range(len(series_to_process)):\n",
    "        splits_list = wordninja.split(series_to_process[i])\n",
    "        combined_string = \" \".join(splits_list)\n",
    "        author_split.append(combined_string)\n",
    "    new_list = []\n",
    "    tokenizer = RegexpTokenizer(r'(\\w+)')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(author_split)):\n",
    "        #TOKENISED ITEM(LONG STRING) IN A LIST\n",
    "        dirty_string = (author_split)[i].lower()\n",
    "        words_only = tokenizer.tokenize(dirty_string) #WORDS_ONLY IS A LIST THAT DOESN'T HAVE PUNCTUATION\n",
    "        #LEMMATISE THE ITEMS IN WORDS_ONLY\n",
    "        words_only_lem = [lemmatizer.lemmatize(i) for i in words_only]\n",
    "        #REMOVING STOP WORDS FROM THE LEMMATIZED LIST\n",
    "        words_without_stop = [i for i in words_only_lem if i not in stopwords.words(\"english\")]\n",
    "        #RETURN SEPERATED WORDS INTO LONG STRING\n",
    "        long_string_clean = \" \".join(word for word in words_without_stop)\n",
    "        new_list.append(long_string_clean)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selftext and title\n",
    "combined_data[\"selftext_clean\"] = processing_text(combined_data[\"selftext\"])\n",
    "combined_data[\"title_clean\"] = processing_text(combined_data[\"title\"])\n",
    "#author\n",
    "combined_data[\"author_clean\"]= processing_author_names(combined_data[\"author\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We understand that most people who reply immediately to an OP with an invitation to talk privately  mean only to help, but this type of response usually leads to either disappointment or disaster.  it usually works out quite differently here than when you say \"PM me anytime\" in a casual social context.  \\n\\nWe have huge admiration and appreciation for the goodwill and good citizenship of so many of you who support others here and flag inappropriate content - even more so because we know that so many of you are struggling yourselves.  We're hard at work behind the scenes on more information and resources to make it easier to give and get quality help here - this is just a small start.  \\n\\nOur new wiki page explains in detail why it's much better to respond in public comments, at least until you've gotten to know someone.  It will be maintained at /r/depression/wiki/private_contact, and the full text of the current version is below.\\n\\n*****\\n\\n###Summary###\\n\\n**Anyone who, while a...</td>\n",
       "      <td>understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private_contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people op respons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  selftext  \\\n",
       "0  We understand that most people who reply immediately to an OP with an invitation to talk privately  mean only to help, but this type of response usually leads to either disappointment or disaster.  it usually works out quite differently here than when you say \"PM me anytime\" in a casual social context.  \\n\\nWe have huge admiration and appreciation for the goodwill and good citizenship of so many of you who support others here and flag inappropriate content - even more so because we know that so many of you are struggling yourselves.  We're hard at work behind the scenes on more information and resources to make it easier to give and get quality help here - this is just a small start.  \\n\\nOur new wiki page explains in detail why it's much better to respond in public comments, at least until you've gotten to know someone.  It will be maintained at /r/depression/wiki/private_contact, and the full text of the current version is below.\\n\\n*****\\n\\n###Summary###\\n\\n**Anyone who, while a...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            selftext_clean  \n",
       "0  understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private_contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people op respons...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check selftext_clean\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "combined_data[[\"selftext\",\"selftext_clean\"]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is \"helpers may not invite private contact as a first...</td>\n",
       "      <td>broken least understood rule helper may invite private contact first resort made new wiki explain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 title  \\\n",
       "0  Our most-broken and least-understood rules is \"helpers may not invite private contact as a first...   \n",
       "\n",
       "                                                                                         title_clean  \n",
       "0  broken least understood rule helper may invite private contact first resort made new wiki explain  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check title_clean\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "combined_data[[\"title\",\"title_clean\"]].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>sql witch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author author_clean\n",
       "0  SQLwitch    sql witch"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check author_clean\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "combined_data[[\"author\",\"author_clean\"]].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further analytics in modeling part, we need to use 'selftext_clean', 'title_clean', and 'author_clean'. Creating a new column named 'megatext' would be more convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is \"helpers may not invite private contact as a first resort\", so we've made a new wiki to explain it</td>\n",
       "      <td>We understand that most people who reply immediately to an OP with an invitation to talk privately  mean only to help, but this type of response usually leads to either disappointment or disaster.  it usually works out quite differently here than when you say \"PM me anytime\" in a casual social context.  \\n\\nWe have huge admiration and appreciation for the goodwill and good citizenship of so many of you who support others here and flag inappropriate content - even more so because we know that so many of you are struggling yourselves.  We're hard at work behind the scenes on more information and resources to make it easier to give and get quality help here - this is just a small start.  \\n\\nOur new wiki page explains in detail why it's much better to respond in public comments, at least until you've gotten to know someone.  It will be maintained at /r/depression/wiki/private_contact, and the full text of the current version is below.\\n\\n*****\\n\\n###Summary###\\n\\n**Anyone who, while a...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/doqwow/our_mostbroken_and_leastunderstood_rules_is/</td>\n",
       "      <td>understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private_contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people op respons...</td>\n",
       "      <td>broken least understood rule helper may invite private contact first resort made new wiki explain</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>sql witch understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private_contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              title  \\\n",
       "0  Our most-broken and least-understood rules is \"helpers may not invite private contact as a first resort\", so we've made a new wiki to explain it   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  selftext  \\\n",
       "0  We understand that most people who reply immediately to an OP with an invitation to talk privately  mean only to help, but this type of response usually leads to either disappointment or disaster.  it usually works out quite differently here than when you say \"PM me anytime\" in a casual social context.  \\n\\nWe have huge admiration and appreciation for the goodwill and good citizenship of so many of you who support others here and flag inappropriate content - even more so because we know that so many of you are struggling yourselves.  We're hard at work behind the scenes on more information and resources to make it easier to give and get quality help here - this is just a small start.  \\n\\nOur new wiki page explains in detail why it's much better to respond in public comments, at least until you've gotten to know someone.  It will be maintained at /r/depression/wiki/private_contact, and the full text of the current version is below.\\n\\n*****\\n\\n###Summary###\\n\\n**Anyone who, while a...   \n",
       "\n",
       "     author  num_comments  is_suicide  \\\n",
       "0  SQLwitch           133           0   \n",
       "\n",
       "                                                                                                url  \\\n",
       "0  https://www.reddit.com/r/depression/comments/doqwow/our_mostbroken_and_leastunderstood_rules_is/   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            selftext_clean  \\\n",
       "0  understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private_contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people op respons...   \n",
       "\n",
       "                                                                                         title_clean  \\\n",
       "0  broken least understood rule helper may invite private contact first resort made new wiki explain   \n",
       "\n",
       "  author_clean  \\\n",
       "0    sql witch   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            megatext_clean  \n",
       "0  sql witch understand people reply immediately op invitation talk privately mean help type response usually lead either disappointment disaster usually work quite differently say pm anytime casual social context huge admiration appreciation goodwill good citizenship many support others flag inappropriate content even know many struggling hard work behind scene information resource make easier give get quality help small start new wiki page explains detail much better respond public comment least gotten know someone maintained r depression wiki private_contact full text current version summary anyone acting helper invite accepts private contact e pm chat kind offsite communication early conversion showing either bad intention bad judgement either way unwise trust pm anytime seems like kind generous offer might perfectly well meaning unless solid rapport ha established wise idea point consider offer accept invitation communicate privately posting supportive reply publicly help people ...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new colum\n",
    "combined_data[\"megatext_clean\"] = combined_data[\"author_clean\"] + \" \" + combined_data[\"selftext_clean\"]+ \" \" +combined_data[\"title_clean\"]\n",
    "#check\n",
    "combined_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename this new dataset as model_data. Then upload it to GitHub and read this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "\n",
       "                                            selftext    author  num_comments  \\\n",
       "0  We understand that most people who reply immed...  SQLwitch           133   \n",
       "\n",
       "   is_suicide                                                url  \\\n",
       "0           0  https://www.reddit.com/r/depression/comments/d...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "\n",
       "                                         title_clean author_clean  \\\n",
       "0  broken least understood rule helper may invite...    sql witch   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import model data\n",
    "model_data = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/data_for_model.csv', keep_default_na=False)\n",
    "model_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the modeling part.\n",
    "## Part 3 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1897 entries, 0 to 1896\n",
      "Data columns (total 12 columns):\n",
      "title              1897 non-null object\n",
      "selftext           1897 non-null object\n",
      "author             1897 non-null object\n",
      "num_comments       1897 non-null int64\n",
      "is_suicide         1897 non-null int64\n",
      "url                1897 non-null object\n",
      "selftext_clean     1897 non-null object\n",
      "title_clean        1897 non-null object\n",
      "author_clean       1897 non-null object\n",
      "selftext_length    1897 non-null int64\n",
      "title_length       1897 non-null int64\n",
      "megatext_clean     1897 non-null object\n",
      "dtypes: int64(4), object(8)\n",
      "memory usage: 177.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#check the model data\n",
    "model_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, define the explanory feature and response variable. Here we only use megatext_clean to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING X and y\n",
    "X = model_data['megatext_clean']\n",
    "y = model_data['is_suicide']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to split the train and test dataset then do the pre-processing with X_trian and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV       \n",
    "#TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try simple one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10524 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "#print(X)\n",
    "\n",
    "sequnces = tokenizer.texts_to_sequences(X_train)\n",
    "#print(sequnces)\n",
    "onehot_X = tokenizer.texts_to_matrix(X_train, mode='binary')\n",
    "#print(one_hot_result)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5393 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "#print(X)\n",
    "\n",
    "sequnces = tokenizer.texts_to_sequences(X_test)\n",
    "#print(sequnces)\n",
    "onehot_X_test = tokenizer.texts_to_matrix(X_test, mode='binary')\n",
    "#print(one_hot_result)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "onehot_X = sequence.pad_sequences(onehot_X, maxlen=20)\n",
    "#onehot_X_test = sequence.pad_sequences(onehot_X_test, maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 0s 6ms/step - loss: 0.6936 - acc: 0.5062 - val_loss: 0.6912 - val_acc: 0.5592\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.5029 - val_loss: 0.6919 - val_acc: 0.5592\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.5062 - val_loss: 0.6932 - val_acc: 0.5461\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6934 - acc: 0.4732 - val_loss: 0.6919 - val_acc: 0.5592\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6934 - acc: 0.4996 - val_loss: 0.6922 - val_acc: 0.5526\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.4831 - val_loss: 0.6912 - val_acc: 0.5592\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6935 - acc: 0.5062 - val_loss: 0.6911 - val_acc: 0.5592\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6934 - acc: 0.4946 - val_loss: 0.6909 - val_acc: 0.5592\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6933 - acc: 0.5062 - val_loss: 0.6925 - val_acc: 0.5526\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - acc: 0.4996 - val_loss: 0.6906 - val_acc: 0.5592\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(onehot_X, y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 32,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1517, 20)\n",
      "Shape of label tensor: (1897,)\n"
     ]
    }
   ],
   "source": [
    "data = sequence.pad_sequences(onehot_X, maxlen=20)\n",
    "\n",
    "labels = y\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1517, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequnces[0]\n",
    "data[0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for values in x_train:\n",
    "    #values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]\n",
    "embedding_matrix[1]\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                64032     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,064,065\n",
      "Trainable params: 1,064,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the GloVe embeddings \n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.6931 - acc: 0.5200 - val_loss: 0.6928 - val_acc: 0.6112\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6929 - acc: 0.5600 - val_loss: 0.6923 - val_acc: 0.6112\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.6926 - acc: 0.5600 - val_loss: 0.6919 - val_acc: 0.6112\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.6924 - acc: 0.5600 - val_loss: 0.6915 - val_acc: 0.6112\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6922 - acc: 0.5600 - val_loss: 0.6913 - val_acc: 0.6112\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.6921 - acc: 0.5600 - val_loss: 0.6911 - val_acc: 0.6112\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6920 - acc: 0.5600 - val_loss: 0.6909 - val_acc: 0.6112\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6919 - acc: 0.5600 - val_loss: 0.6906 - val_acc: 0.6112\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6918 - acc: 0.5600 - val_loss: 0.6904 - val_acc: 0.6112\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.6917 - acc: 0.5600 - val_loss: 0.6900 - val_acc: 0.6112\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " without pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                64032     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,064,065\n",
      "Trainable params: 1,064,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.7036 - acc: 0.5200 - val_loss: 0.6922 - val_acc: 0.6128\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6925 - acc: 0.5600 - val_loss: 0.6910 - val_acc: 0.6112\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6918 - acc: 0.5600 - val_loss: 0.6897 - val_acc: 0.6112\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6911 - acc: 0.5600 - val_loss: 0.6882 - val_acc: 0.6112\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.6907 - acc: 0.5600 - val_loss: 0.6867 - val_acc: 0.6112\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.6901 - acc: 0.5600 - val_loss: 0.6849 - val_acc: 0.6112\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6890 - acc: 0.5600 - val_loss: 0.6833 - val_acc: 0.6112\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 0.6886 - acc: 0.5600 - val_loss: 0.6817 - val_acc: 0.6112\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.6877 - acc: 0.5600 - val_loss: 0.6794 - val_acc: 0.6112\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.6879 - acc: 0.5600 - val_loss: 0.6795 - val_acc: 0.6112\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(max_words, embedding_dim, input_length=20))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(optimizer='rmsprop', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "history = model1.fit(x_train, y_train, \n",
    "                    epochs=10, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = sequence.pad_sequences(sequences, maxlen=20)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 380\n  y sizes: 1517\nPlease provide data which shares the same first dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-afe2bb6e97ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pre_trained_glove_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\programming\\9.3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming\\9.3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1354\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming\\9.3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programming\\9.3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[0;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 380\n  y sizes: 1517\nPlease provide data which shares the same first dimension."
     ]
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.6642 - acc: 0.6687 - val_loss: 0.4830 - val_acc: 0.9750\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.3976 - acc: 1.0000 - val_loss: 0.2575 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2077 - acc: 1.0000 - val_loss: 0.1610 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1345 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1015 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0818 - acc: 1.0000 - val_loss: 0.0779 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0683 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0581 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0501 - acc: 1.0000 - val_loss: 0.0486 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use __LSTM__ to build the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,21))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 0s 211ms/step - loss: 0.6898 - acc: 0.5813 - val_loss: 0.6621 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6576 - acc: 1.0000 - val_loss: 0.6269 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6179 - acc: 1.0000 - val_loss: 0.5738 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.5570 - acc: 1.0000 - val_loss: 0.4850 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4546 - acc: 1.0000 - val_loss: 0.3326 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2848 - acc: 1.0000 - val_loss: 0.1417 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1048 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0377 - acc: 1.0000 - val_loss: 0.0296 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

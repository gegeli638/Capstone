{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>is_suicide</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_clean</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>author_clean</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>megatext_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our most-broken and least-understood rules is ...</td>\n",
       "      <td>We understand that most people who reply immed...</td>\n",
       "      <td>SQLwitch</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/depression/comments/d...</td>\n",
       "      <td>understand people reply immediately op invitat...</td>\n",
       "      <td>broken least understood rule helper may invite...</td>\n",
       "      <td>sql witch</td>\n",
       "      <td>4792</td>\n",
       "      <td>144</td>\n",
       "      <td>sql witch understand people reply immediately ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Our most-broken and least-understood rules is ...   \n",
       "\n",
       "                                            selftext    author  num_comments  \\\n",
       "0  We understand that most people who reply immed...  SQLwitch           133   \n",
       "\n",
       "   is_suicide                                                url  \\\n",
       "0           0  https://www.reddit.com/r/depression/comments/d...   \n",
       "\n",
       "                                      selftext_clean  \\\n",
       "0  understand people reply immediately op invitat...   \n",
       "\n",
       "                                         title_clean author_clean  \\\n",
       "0  broken least understood rule helper may invite...    sql witch   \n",
       "\n",
       "   selftext_length  title_length  \\\n",
       "0             4792           144   \n",
       "\n",
       "                                      megatext_clean  \n",
       "0  sql witch understand people reply immediately ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import model data\n",
    "model_data = pd.read_csv('https://raw.githubusercontent.com/gegeli638/Capstone/master/data_for_model.csv', keep_default_na=False)\n",
    "model_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINING X and y\n",
    "X = model_data['megatext_clean'].tolist()\n",
    "y = model_data['is_suicide'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV       \n",
    "#TRAIN-TEST SPLIT\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10524 unique tokens.\n",
      "Shape of data tensor: (1517, 100)\n",
      "Shape of label tensor: (1517,)\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100 #Cuts off reviews after 100 words\n",
    "training_samples = 1500\n",
    "validation_samples = 50\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(tokenizer.word_index)\n",
    "\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(Y_train)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir = 'glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'), encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71187001 -0.34548     0.25773999  1.11580002 -0.45910001 -1.13559997\n",
      " -0.49160001 -0.41088    -0.82639998  0.14788     0.017755    0.4738\n",
      "  0.4341     -0.75437999 -1.1415      0.32315999 -0.10246     0.27882999\n",
      "  0.98781002  1.87709999 -0.85609001 -0.072251    0.79453999  0.32765999\n",
      " -0.29482999 -0.38997999 -0.67232001 -0.18064    -0.57815999 -0.85960001\n",
      "  0.43899    -0.086074   -0.95765001  0.71298999  0.80085999  0.048109\n",
      "  0.09286    -1.01240003  0.13322    -0.25224    -0.26030999 -0.28819001\n",
      " -0.67439002 -1.15820003  0.28542     0.44405001 -0.72180998  0.24398001\n",
      "  1.42970002 -0.2545    ]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix[0]\n",
    "embedding_matrix[1]\n",
    "embedding_matrix.shape\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 660,065\n",
      "Trainable params: 660,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "#model.add(LSTM(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.6981 - acc: 0.5840 - val_loss: 0.6451 - val_acc: 0.5294\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.5273 - acc: 0.7440 - val_loss: 0.7614 - val_acc: 0.5882\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.3631 - acc: 0.8640 - val_loss: 0.7519 - val_acc: 0.5294\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.2217 - acc: 0.9413 - val_loss: 0.8170 - val_acc: 0.7059\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 8ms/step - loss: 0.1246 - acc: 0.9740 - val_loss: 0.9009 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 9ms/step - loss: 0.0678 - acc: 0.9893 - val_loss: 1.4205 - val_acc: 0.5294\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0479 - acc: 0.9940 - val_loss: 1.3435 - val_acc: 0.4118\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0288 - acc: 0.9967 - val_loss: 1.8719 - val_acc: 0.5294\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 6ms/step - loss: 0.0146 - acc: 0.9980 - val_loss: 2.1183 - val_acc: 0.4118\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 7ms/step - loss: 0.0208 - acc: 0.9967 - val_loss: 1.9616 - val_acc: 0.4118\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 1.3880 - acc: 0.5947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3880292177200317, 0.5947368144989014]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                10624     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 510,657\n",
      "Trainable params: 510,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "#model.add(Embedding(10000,32))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "#history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 0.6975 - acc: 0.4867 - val_loss: 0.6545 - val_acc: 0.5882\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6885 - acc: 0.5400 - val_loss: 0.6538 - val_acc: 0.8235\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6840 - acc: 0.5740 - val_loss: 0.6411 - val_acc: 0.5882\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6795 - acc: 0.5667 - val_loss: 0.6405 - val_acc: 0.7059\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6753 - acc: 0.5940 - val_loss: 0.6322 - val_acc: 0.5882\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6716 - acc: 0.5820 - val_loss: 0.6304 - val_acc: 0.7647\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6682 - acc: 0.6093 - val_loss: 0.6239 - val_acc: 0.6471\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6649 - acc: 0.6027 - val_loss: 0.6226 - val_acc: 0.7059\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6616 - acc: 0.6240 - val_loss: 0.6156 - val_acc: 0.6471\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6585 - acc: 0.6153 - val_loss: 0.6178 - val_acc: 0.7059\n"
     ]
    }
   ],
   "source": [
    "#Compile and train the model\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size= 1517, validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 6ms/step - loss: 0.6647 - acc: 0.5789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.664656937122345, 0.5789473652839661]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
